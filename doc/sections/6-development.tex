% Template for a Thesis
%
% 6-development.tex
%
% Development

\chapter{Development}\label{sec:development}

This chapter talks more deeply about the code and algorithms previously explained in the Methodology section.~\ref{sec:methodology}

\section{Coding practices}

The parameters for the pipeline, such as num\_symbols, dropout, file paths, etc. have been written in \emph{settings.py}.

\begin{python}
# global variables
import os
from os.path import join
import sys

word_sep = u'\u2581'
source, target = 'eng', 'deu'

num_all_symbols = 20000
all_symbols = [100, 200, 500, 1000, 2000, 4000, 6000, 8000]

rootdir = os.getcwd()
if rootdir.split(os.sep)[-1] == 'src':
    rootdir = os.sep.join(rootdir.split(os.sep)[:-1])

datadir = join(rootdir, 'data')
inputdir = join(datadir, 'input')
bpedir = join(datadir, 'dropout_bpe' if dropout > 0 else 'normal_bpe')
baselinedir = join(rootdir, 'reports', 'scores_normal_bpe')
scoredir = join(rootdir, 'reports', 'scores_' + ('dropout_bpe' if dropout > 0 else 'normal_bpe'))
goldpath = join(inputdir, 'eng_deu.gold')
inputpath = {source: join(inputdir, source+'_with_10k.txt'),
            target: join(inputdir, target+'_with_10k.txt')}

fastalign_path = join(rootdir, "tools/fast_align/build/fast_align")
atools_path = join(rootdir, "tools/fast_align/build/atools")
    
\end{python}

\section{Replication of BPE results}

\begin{enumerate}
    \item Write learn BPE from corpus algorithm
    \item Write apply BPE to corpus algorithm
    \item Write extract alignment script
    \item Write calculate alignment scores script
\end{enumerate}

\subsection{Learn BPE algorithm}

\begin{python}
#!/usr/bin/env python

import os
import re
import sys
import codecs
from tqdm import tqdm
from os.path import join
from collections import defaultdict, Counter

# import global variables from settings.py
sys.path.insert(1, os.path.join(sys.path[0], '..'))
from settings import *

def read_corpus(corpus: list) -> list:
  """
  Read corpus, strip index and new line characters.
  In space mode, each word has a word_sep symbol at the beginning to signal it's the beginning of the word.
  example:
  tokens = [
      '\_w e \_d o \_n o t \_b e l i e v e 
      \_t h a t \_w e \_s h o u l d 
      \_c h e r r y - p i c k \_.',
      ...
  ]
  """

  tokens = []
  for line in corpus:
    line = line.split('\t')[1].strip('\r\n ')
    line = line.split()
    line[0] = str.lower(line[0])

    # add word_sep to each beginning of word and join by space
    tokens.append(' '.join([word_sep + ' '.join(word) for word in line]))

  return tokens

def get_stats(tokens: list) -> Counter:
  """
  Count frequency of all bigrams and the frequency per index.
  pairs = {
    ('s', 'h'): 5,
    ('h', 'e'): 6
  }
  The last token '.' or word_sep. isn't merged with anything.
  """

  pairs = Counter()
  for i, sent in enumerate(tokens):
    # get stats for each word independently, 
    # no bigrams between different words
    for word in sent[1:].split(' '+word_sep):
      symbols = symbols.split()
      for j in range(len(symbols) - 1):
        pairs[symbols[j], symbols[j + 1]] += 1

  return pairs


def merge_token(corpus, most_frequent):
  str_corpus = '\n'.join(corpus)
  str_corpus = str_corpus.replace(' '.join(most_frequent), ''.join(most_frequent))
  return str_corpus.split('\n')


def learn_bpe(argsinput, bpe_model):
  """
  Learn BPE operations from vocabulary.
  Steps:
  1. split corpus into characters, count frequency
  2. count bigrams in corpus
  3. merge most frequent symbols
  4. Update bigrams in corpus 
  """

  corpus = read_corpus(argsinput)

  most_frequent_merges = []
  for i in range(num_all_symbols):

    pairs = get_stats(corpus)

      try:
        most_frequent = pairs.most_common(1)[0][0]
      except:
        # pairs is empty
        break

      most_frequent_merges.append(most_frequent)
      corpus = merge_token(corpus, most_frequent)

  return most_frequent_merges


def write_bpe(lang, most_freq_merges):

  bpe_file = codecs.open(join(datadir, lang+'.model'), 'w', encoding='utf-8')
  bpe_file.write(f"{lang} {len(most_freq_merges)}\n")
  bpe_file.write('\n'.join(' '.join(item) for item in most_freq_merges))
  return

if __name__ == '__main__':

  for lang in [source, target]:

    argsinput = codecs.open(inputpath[lang], encoding='utf-8')
    bpe_model = codecs.open(join(datadir, lang+'.model'), 'w', encoding='utf-8')
    most_freq_merges = learn_bpe(argsinput, bpe_model)
    write_bpe(lang, most_freq_merges)
\end{python}

\subsection{Apply BPE algorithm}

\begin{python}
# apply_bpe.py

import os
from os.path import join
import sys
import codecs
import random
from tqdm import tqdm

# import global variables from settings.py
sys.path.insert(1, os.path.join(sys.path[0], '..'))
from settings import *
from learn_bpe import read_bpe_model, read_corpus

def load_data():

  os.chdir(datadir)
  langs = [source, target]
  bpe_models = []
  corpora = []
  for lang in langs:

    argsinput = codecs.open(inputpath[lang], encoding='utf-8')
    corpora.append(read_corpus(argsinput))

    bpe_model, _ = read_bpe_model(lang)
    if not bpe_model:
      print(f"No model found for lang={lang}")

    bpe_model = [tuple(item.strip('\r\n ').split(' ')) for (n, item) in enumerate(bpe_model)]
    bpe_models.append(bpe_model[1:])

  return langs, bpe_models, corpora

def write_bpe(lang, num_symbols, merged_corpus):
  outputpath = join(bpedir, 'segmentations', f"{lang}_{num_symbols}.bpe")
  argsoutput = codecs.open(outputpath, 'w', encoding='utf-8')
  argsoutput.write(merged_corpus)
  return

def apply_bpe(langs, bpe_models, corpora):
    
  for lang, bpe_model, corpus in zip(langs, bpe_models, corpora):

    bpe_model = bpe_model[:max(all_symbols)]
    all_symbols_copy = all_symbols.copy()

    str_corpus = '\n'.join(corpus)
    for j, bigram in enumerate(bpe_model):

      str_corpus = str_corpus.replace(' '.join(bigram), ''.join(bigram))

      if j + 1 == all_symbols_copy[0]:
        write_bpe(lang, all_symbols_copy.pop(0), str_corpus)
  return

if __name__ == "__main__":

  os.makedirs(join(bpedir, 'segmentations'), exist_ok=True)
  langs, bpe_models, corpora = load_data()
  apply_bpe()
\end{python}

\subsection{Extract alignments}

In the next step, the extract alignments script takes two files as input: English BPE file, German BPE file, and outputs an alignment file, with the extension .wgdfa.

First of all, it's necessary to iterate through the different merge types that have been done before. There are BPE files with 100 merges, 200, 500, etc for both languages. At each iteration, a different alignment file is created.

Alignment algorithms work on parallel data, that is, they expect text in the following format:

\begin{quote}
  Hello from England ||| Hallo aus Deutschland
\end{quote}

Since the BPE files don't have this format, first of all the function \emph{create\_parallel\_text} creates a \emph{.txt} file in the appropriate format. Afterwards, both \emph{fastalign} and \emph{eflomal} generate forward and reverse alignments. This is handled by the \emph{create\_fwd\_rev\_files} function, which creates \emph{.fwd} and \emph{.rev} files. Afterwards, given these \emph{.fwd} and \emph{.rev} files, the alignment algorithm creates a type of union between these two, called \emph{grow-diag-final-and}, with the extension \emph{.gdfa}. This is handled by the \emph{create\_gdfa\_file} function.

As explained in the \textit{Extract alignment} subsection in the Methodology chapter~\ref{subsec:extractalign}, the script up until now has only aligned BPE units. Those alignments need to be transformed into word alignments. The function \emph{load\_and\_map\_segmentations} loads the BPE files and maps each BPE unit to its corresponding word. For an example, see the comments on the function. This is an auxiliary function in order to map the alignments later. Afterwards, by calling \emph{bpe\_word\_align}, the mapping from subword alignments to word alignments is made. Lastly, the new alignments are saved in a file with the extension \emph{.wgdfa}.

The alignment algorithm is run for two types of files. Firstly, for the corpus itself, the fastalign/eflomal algorithm is run to align the corpus, which will serve as baseline to check how good the BPE merges are. And then, the fastalign/eflomal algorithm is run for the BPE files themselves.

\begin{python}
# extract_alignments.py

from os.path import join
import os
import sys
import codecs

# import global variables from settings.py
sys.path.insert(1, os.path.join(sys.path[0], '..'))
from settings import *
from subword_word import *

def create_parallel_text(sourcepath, targetpath, outpath):

  fa_file = codecs.open(outpath + '.txt', "w", "utf-8")
  fsrc = codecs.open(sourcepath, "r", "utf-8")
  ftrg = codecs.open(targetpath, "r", "utf-8")

  for sl, tl in zip(fsrc, ftrg):
    sl = sl.strip().split("\t")[-1]
    tl = tl.strip().split("\t")[-1]
    fa_file.write(f"{sl} ||| {tl}\n")
  fa_file.close()
  return

def create_fwd_rev_files(outpath):
  if mode == "fastalign":
    os.system(f"{fastalign_path} -i {outpath}.txt -v -d -o > {outpath}.fwd")
    os.system(f"{fastalign_path} -i {outpath}.txt -v -d -o -r > {outpath}.rev")
  elif mode == "eflomal":
    os.system(f"cd {eflomal_path}; python align.py -i {outpath}.txt --model 3 -f {outpath}.fwd -r {outpath}.rev")
  return

def create_gdfa_file(outpath):
  # create gdfa file from .fwd and .rev
  os.system(f"{atools_path} -i {outpath}.fwd -j {outpath}.rev -c grow-diag-final-and > {outpath}_unnum.gdfa")

  # parse _unnum.gdfa to .gdfa with "\t" separator
  with codecs.open(f"{outpath}_unnum.gdfa", "r", "utf-8") as fi, codecs.open(f"{outpath}.gdfa", "w", "utf-8") as fo:
    for i, line in enumerate(fi):
      fo.write(f"{i}\t{line.strip()}\n")

  # delete unnecessary files
  os.system(f"rm {outpath}_unnum.gdfa; rm {outpath}.fwd; rm {outpath}.rev; rm {outpath}.txt")
  return

def load_and_map_segmentations(num_symbols):

    '''
    Given a .bpe file composed of the corpus made of subword units such as
    corpus_eng =  [
        '_We _do _no t _be li eve _.',
        '_Thi s _is _a _sent ence _.',
        ...
    ]
    Output: dictionary of each language and 
    a list of indexes pointing to which word each element (_do) belongs to
    bpes = {
        'eng':
        [
            [0, 1, 2, 2, 3, 3, 3, 4],
            [0, 0, 1, 2, 3, 4, 5],
            ...
        ],
        'deu':
        [
            ...
        ],
    } 
    '''

    bpes = {}
    for lang in [source, target]:

        bpes[lang] = []
        corpus = codecs.open(lang+'_'+str(num_symbols)+'.bpe', encoding='utf-8')
        for sent in corpus:
          mapping = [0]
          i = 0
          for subw in sent.split()[1:]:
              if subw[0] == word_sep:
                i += 1
              mapping.append(i)
          bpes[lang].append(mapping)
    return bpes

def bpe_word_align(bpes, bpe_aligns):
    '''
    Input: dictionary of bpes obtained as output of map_subword_to_word()
    Output: list of word alignments and their indexes
        "
            0   0-0 0-1 1-1 1-2 3-1 2-4 \n
            1   0-0 1-0 1-1 2-1 \n
            ...
        "
    '''
    all_word_aligns = ''
    for i, (sent1, sent2, bpe_al) in enumerate(zip(bpes[source], bpes[target], bpe_aligns)):
        word_aligns = set()
        # iterate each alignment
        for al in bpe_al.split('\t')[1].split():
            firstal, secondal = al.split('-')
            new_al = str(sent1[int(firstal)]) + '-' + str(sent2[int(secondal)])
            word_aligns.add(new_al)
        all_word_aligns += str(i) + "\t" + ' '.join(word_aligns) + "\n"
    return all_word_aligns

def extract_alignments(input_mode=False):

  for num_symbols in all_symbols:

    if input_mode:
      print("Alignments for input files")
      sourcepath = inputpath[source]
      targetpath = inputpath[target]
      outpath = join(bpedir, mode, "input")
    else:
      print(f"Alignments for {num_symbols} symbols")
      sourcepath = join(bpedir, 'segmentations', f"{source}_{num_symbols}.bpe")
      targetpath = join(bpedir, 'segmentations', f"{target}_{num_symbols}.bpe")
      outpath = join(bpedir, mode, str(num_symbols))

    create_parallel_text(sourcepath, targetpath, outpath)
    create_fwd_rev_files(outpath)
    create_gdfa_file(outpath)

    # map alignment from subword to word
    bpes = load_and_map_segmentations(num_symbols)

    argsalign = codecs.open(o+'.gdfa', encoding='utf-8')
    all_word_aligns = bpe_word_align(bpes, argsalign)
    os.system(f"rm {outpath}.gdfa")

    argsoutput = codecs.open(outpath+'.wgdfa', 'w', encoding='utf-8')
    argsoutput.write(all_word_aligns)

  return

if __name__ == "__main__":

  os.makedirs(join(bpedir, mode), exist_ok=True)
  if not os.path.isfile(join(bpedir, mode, 'input.wgdfa')):
    extract_alignments(input_mode=True)

  extract_alignments()
\end{python}

\subsection{Calculate alignment scores}

The last script calculates the alignment scores. These are the steps of the script:

\begin{enumerate}
  \item Load gold dataset
  \item Calculate precision, recall, F1 score and AER metrics
  \item Plot and save into \emph{.png} and \emph{.csv}
\end{enumerate}

\begin{python}
# calc_align_scores.py

import os
from os.path import join
import sys
import glob
import random
import collections
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# import global variables from settings.py
sys.path.insert(1, os.path.join(sys.path[0], '..'))
from settings import *

def load_gold(g_path):

  gold_f = open(g_path, "r")
  pros = {}
  surs = {}
  all_count = 0.
  surs_count = 0.

  for line in gold_f:
    line = line.strip().split("\t")
    line[1] = line[1].split()

    pros[line[0]] = set()
    surs[line[0]] = set()

    for al in line[1]:
      pros[line[0]].add(al.replace('p', '-'))
      if 'p' not in al:
        surs[line[0]].add(al)

    all_count += len(pros[line[0]])
    surs_count += len(surs[line[0]])

  return pros, surs, surs_count

def calc_score(input_path, probs, surs, surs_count):

  total_hit = 0.
  p_hit = 0.
  s_hit = 0.
  target_f = open(input_path, "r")

  for line in target_f:
    line = line.strip().split("\t")

    if line[0] not in probs: continue
    if len(line) < 2: continue

    line[1] = line[1].split()
    for pair in line[1]:
      if pair in probs[line[0]]:
        p_hit += 1
      if pair in surs[line[0]]:
        s_hit += 1

      total_hit += 1

  target_f.close()

  y_prec = round(p_hit / max(total_hit, 1.), 3)
  y_rec = round(s_hit / max(surs_count, 1.), 3)
  y_f1 = round(2. * y_prec * y_rec / max((y_prec + y_rec), 0.01), 3)
  aer = round(1 - (s_hit + p_hit) / (total_hit + surs_count), 3)

  return y_prec, y_rec, y_f1, aer

def get_baseline_score(probs, surs, surs_count):

  alfile = join(bpedir, mode, 'input.wgdfa')

  scores = []
  score = [0]
  score.extend(list(calc_score(alfile, probs, surs, surs_count)))
  scores.append(score)
  baseline_df = pd.DataFrame(scores, columns=['num_symbols', 'prec', 'rec', 'f1', 'AER']).round(decimals=3)
  return baseline_df

def calc_align_scores(probs, surs, surs_count):

  scores = []
  for num_symbols in all_symbols:
    alfile = join(bpedir, mode, f"{num_symbols}.wgdfa")
    
    score = [int(num_symbols)]
    score.extend(list(calc_score(alfile, probs, surs, surs_count)))
    scores.append(score)

  df = pd.DataFrame(scores, columns=['num_symbols', 'prec', 'rec', 'f1', 'AER']).round(decimals=3)
  return df


def plot_scores(df, baseline_df, scoredir):

  # Use plot styling from seaborn.
  sns.set(style='darkgrid')

  # Increase the plot size and font size.
  sns.set(font_scale=1.5)
  plt.rcParams["figure.figsize"] = (12, 6)

  plt.clf()
  ax = plt.gca() # gca stands for 'get current axis'

  colors = ['magenta', 'tab:blue', 'tab:green', 'tab:red']

  df = df.sort_values('num_symbols')
  columns = list(df)
  for column, color in zip(columns[1:], colors):
    df.plot(kind='line', x=columns[0], y=column, color=color, ax=ax)

  for baseline_results, color in zip(list(baseline_df.iloc[0][1:]), colors):
    plt.axhline(y=baseline_results, color=color, linestyle='dashed')

  plt.savefig(join(scoredir+'.png'))
  return


if __name__ == "__main__":
  '''
  Calculate alignment quality scores based on the gold standard.
  The output contains Precision, Recall, F1, and AER.
  '''

  probs, surs, surs_count = load_gold(goldpath)
  baseline_df = get_baseline_score(probs, surs, surs_count)
  df = calc_align_scores(probs, surs, surs_count, baseline_df)

  scorename = join(scoredir, 'scores')
  print(f"Scores saved into {scorename}")
  df.to_csv(scorename+'.csv', index=False)
  plot_scores(df, baseline_df, scorename)

\end{python}

\section{Replication of BPE dropout}

The previous section has laid the backbone of the algorithms. In this and the following sections, some modifications are introduced. For the sake of simplicity, the code snippets that follow only include the new changes, or the functions from the last section with new modifications. The functions that remain unaltered aren't shown.

Two new parameters come into play in the pipeline, namely \textbf{dropout} rate and \textbf{dropout\_samples}, that is, how many samples of the dropout system are considered. These values are saved into \emph{settings.py}

\begin{python}
# settings.py

dropout = 0.1
dropout_samples = 10
\end{python}

\subsection{Apply BPE to corpus with dropout}

The first modifications are in \emph{apply\_bpe.py}, where we skip some merges, and repeat the process 10 times. The function \emph{apply\_bpe} includes two new lines where a random number between 0 and 1 is generated. If this number is smaller than the \emph{dropout} rate saved in \emph{settings.py}, then that merge isn't considered and the loop skips it. Additionally, in the main function, the function \emph{apply\_bpe} is called \emph{dropout\_samples} times. To save the files accordingly, a new variable is introduced, namely \emph{i}, that does nothing in the case where dropout=0, but when repeating the process, for instance if lang=eng, num\_symbols=2000, and first iteration of dropout, that is, i=0, the files are saved as \emph{eng\_2000\_0.bpe} instead.

\begin{python}
# apply_bpe.py
import random

def write_bpe(lang, num_symbols, merged_corpus, i=-1):

  outputpath = join(bpedir, 'segmentations', f"{lang}_{num_symbols}{f'_{i}' if i != -1 else ''}.bpe")
  argsoutput = codecs.open(outputpath, 'w', encoding='utf-8')
  argsoutput.write(merged_corpus)
  return

def apply_bpe(langs, bpe_models, corpora, i=-1):
    
  for lang, bpe_model, corpus in zip(langs, bpe_models, corpora):

    bpe_model = bpe_model[:max(all_symbols)]
    all_symbols_copy = all_symbols.copy()
    str_corpus = '\n'.join(corpus)

    for j, bigram in enumerate(bpe_model):

        if random.uniform(0, 1) < dropout:
            continue

      str_corpus = str_corpus.replace(' '.join(bigram), ''.join(bigram))

      if j + 1 == all_symbols_copy[0]:
        write_bpe(lang, all_symbols_copy.pop(0), str_corpus, i)
  return

if __name__ == "__main__":

  langs, bpe_models, corpora = load_data()

  if dropout > 0:
    for i in range(dropout_samples):
      apply_bpe(i)
  else:
      apply_bpe()
\end{python}

\subsection{Extract alignments with dropout}

The only change here is that the \emph{extract\_alignment} function is called \emph{dropout\_samples} times, which changes the function to write the alignments in the new format, namely, changing the variables \emph{sourcepath} and \emph{targetpath}. The rest, the alignment algorithm, remains unchanged.

\begin{python}
# extract_alignments.py

def extract_alignments(i=-1, input_mode=False):

  for num_symbols in all_symbols:

    if input_mode:
      print("Alignments for input files")
      sourcepath = inputpath[source]
      targetpath = inputpath[target]
      outpath = join(bpedir, mode, "input")
    else:
      print(f"Alignments for {num_symbols} symbols")
      sourcepath = join(bpedir, 'segmentations', f"{source}_{num_symbols}_{'_'+str(i) if dropout else ''}.bpe")
      targetpath = join(bpedir, 'segmentations', f"{target}_{num_symbols}_{'_'+str(i) if dropout else ''}.bpe")
      outpath = join(bpedir, mode, str(num_symbols))

    create_parallel_text(sourcepath, targetpath, outpath)
    create_fwd_rev_files(outpath)
    create_gdfa_file(outpath)

    # map alignment from subword to word
    bpes = load_and_map_segmentations(num_symbols)

    argsalign = codecs.open(o+'.gdfa', encoding='utf-8')
    all_word_aligns = bpe_word_align(bpes, argsalign)
    os.system(f"rm {outpath}.gdfa")

    argsoutput = codecs.open(outpath+'.wgdfa', 'w', encoding='utf-8')
    argsoutput.write(all_word_aligns)

  return

if __name__ == "__main__":

  os.makedirs(join(bpedir, mode), exist_ok=True)
  if not os.path.isfile(join(bpedir, mode, 'input.wgdfa')):
    extract_alignments(input_mode=True)

  if dropout > 0:
    for i in range(dropout_samples):
      extract_alignments(i)
  else:
    extract_alignments()
\end{python}

\subsection{Calculate alignment scores with dropout}

As explained in the Methodology section~\ref{sec:replbpedrop}, variants of union, intersection and threshold are created. This is introduced with a new algorithm, namely \emph{merge\_dropout.py}. First of all, the function \emph{merge\_dropout\_alignments} opens all alignment files and creates a dictionary data structure with the union, intersection and threshold alignment files and saves them into \emph{X\_union.wgdfa}, \emph{X\_inter.wgdfa}, \emph{X\_thres.wgdfa} respectively. Afterwards, the function \emph{calc\_score\_merges} opens these files and calculates the score, much in the way as the \emph{calc\_align\_score} algorithm from the previous section.

\begin{python}
# merge_dropout.py
import os
from os.path import join
import sys
import codecs
import pandas as pd
from tqdm import tqdm
from collections import Counter

# import global variables from settings.py
sys.path.insert(1, os.path.join(sys.path[0], '..'))
from settings import *
from calc_align_score import *

def merge_dropout_alignments():
  union_merge, inter_merge, thres_merge = {}, {}, {}

  os.chdir(join(bpedir, mode))
  for num_symbols in tqdm(all_symbols, desc=f"merge_dropout: dropout={dropout}, union, inter, thres"):
    union_merge[num_symbols], inter_merge[num_symbols], thres_merge[num_symbols] = [], [], []

    for i in range(dropout_sampless):

      for j, line in enumerate(open(f'{num_symbols}_{i}.wgdfa', 'r').readlines()):
        al = frozenset(line.strip().split("\t")[1].split())

        # at the first iteration, just append the alignment
        if i == 0:
          union_merge[num_symbols].append(al)
          inter_merge[num_symbols].append(al)
          thres_merge[num_symbols].append(Counter(al))
          continue
        
        # do union, intersection or frequency addition
        union_merge[num_symbols][j] |= al
        inter_merge[num_symbols][j] &= al
        thres_merge[num_symbols][j] += Counter(al)

    # write to output
    unionfile = codecs.open(f'{num_symbols}_union.wgdfa', 'w')
    interfile = codecs.open(f'{num_symbols}_inter.wgdfa', 'w')
    thresfiles = {merge_t: codecs.open(f'{num_symbols}_thres_{merge_t}.wgdfa', 'w') for merge_t in merge_threshold}

    for i in range(len(union_merge[num_symbols])):
      unionfile.write(f"{i}\t{' '.join(union_merge[num_symbols][i])}\n")
      interfile.write(f"{i}\t{' '.join(inter_merge[num_symbols][i])}\n")

      # get alignments more common than the merge_threshold %
      for merge_t in merge_threshold:
        common_aligns = [k for k in thres_merge[num_symbols][i] 
                        if thres_merge[num_symbols][i][k] > merge_t * dropout_samples]
        thresfiles[merge_t].write(f"{i}\t{' '.join(common_aligns)}\n")
  return


def calc_score_merges():
  probs, surs, surs_count = load_gold(goldpath)
  baseline_df = pd.read_csv(join(baselinedir, f'scores_{source}_{target}.csv'))
  scorespath = join(scoredir, str(dropout))
  if not os.path.isdir(scorespath):
      os.mkdir(scorespath)

  for merge_type in ['union', 'inter']:
    scores = []
    for num_symbols in all_symbols:
      mergefilepath = join(bpedir, mode, f'{num_symbols}_{merge_type}.wgdfa')
      score = [int(num_symbols)]
      score.extend(list(calc_score(mergefilepath, probs, surs, surs_count)))
      scores.append(score)

    df = pd.DataFrame(scores, columns=['num_symbols', 'prec', 'rec', 'f1', 'AER']).round(decimals=3)
    scorename = join(scorespath, 'scores', merge_type)

    print(f"Scores saved into {scorename}")
    df.to_csv(scorename+'.csv', index=False)
    plot_scores(df, baseline_df, scorename)

  # threshold case, iterate all merge_thresholds saved
  for merge_t in merge_threshold:
    scores = []
    for num_symbols in all_symbols:
      mergefilepath = join(bpedir, mode, f'{num_symbols}_thres_{merge_t}.wgdfa')
      score = [int(num_symbols)]
      score.extend(list(calc_score(mergefilepath, probs, surs, surs_count)))
      scores.append(score)

    df = pd.DataFrame(scores, columns=['num_symbols', 'prec', 'rec', 'f1', 'AER']).round(decimals=3)
    scorename = join(scorespath, 'scores', f"{merge_t}_thres")
    
    print(f"Scores saved into {scorename}")
    df.to_csv(scorename+'.csv', index=False)
    plot_scores(df, baseline_df, scorename)
  return

if __name__ == "__main__":
    merge_dropout_alignments()
    calc_score_merges()
\end{python}


\begin{python}

\end{python}
