% Template for a Thesis
%
% 1-introduction.tex
%
% Introduction

\chapter{Introduction}\label{sec:introduction}

Natural language processing, NLP, is the task of processing natural language, transforming it, obtaining information from it, classifying it, translating it, and many other tasks. It is closely related to NLU, natural language understanding, which seeks to more theoretically understand the meaning of language. It could be said that linguistics is more involved in NLU, whereas computer science, algorithms and programming are the backbone of NLP.

There has been great progress in NLP since around 2017 until the time of writing, 2020, mostly driven by computational advances. Pre-2017 NLP mostly used convolutional or recurrent neural networks, built them from scratch with hundreds of thousands of hyperparameters and trained them on CPUs locally. The datasets used for these models were comparable in size with the models. In 2017, Vaswani et al~\cite{vaswani2017attention} introduced a model neural network architecture, Transformers, that removed any sequentiallity and focused solely on attention. This architecture is very powerful and soon thereafter, BERT~\cite{devlin2018bert} was introduced, which enabled the pre-training of deep bidirectional Transformers. BERT is very computationally expensive and requires intensive training, which is why the authors published pre-trained weights on many NLP tasks that users could later fine tune for their own applications. This introduced the era of pre-training, when users do not train models from scratch anymore and use huge, already pre-trained models. Anyone who wishes to train state-of-the-art models at the time of writing requires very powerful GPUs or even TPUs, which are only available to large organizations given their enormous cost, and several days or even weeks of training, which produces a gigantic energy consumption, as a study from 2019 suggests~\cite{strubell2019energy}. Bigger datasets, bigger models, better embeddings have been trained and published, and they are more power hungry every year. Neural networks, especially the ones being used nowadays, given their size, require vast amounts of data in order to process and adapt their billions of hyperparameters.

It could be said that most of the advances in the past 3-5 years have been computational, and not so much linguistic. The deep learning pipeline has barely changed. First of all, a corpus is read and parsed. Then, embeddings are applied to the parsed corpus and fed to the neural network. This in turn is trained for a number of epochs, the weights typically saved for posteriority, and then the model is evaluated against a test set. Independently of the size or architecture of the neural network, the training time and the relatively new addition of embeddings, the main idea of machine learning has not changed.

This thesis does not introduce any new parameter in the main pipeline, but rather focuses on the corpus-embeddings step and explores how the corpus is parsed so that the embeddings are optimally applied to it. The definition and usage of embeddings is explained in the literature review section of this thesis, but in broad terms, embeddings are a mapping between characters and numbers. Computers cannot work with pictures, audio or language, they only process numbers, which is why for all data types, they first need to be transformed into numbers. How to make this transformation depends at great extent on the type of data: for instance, sequentiallity is very important in audio and language. An utterance or word generally loses its meaning when it is displaced in time. In images, however, the important metric is knowing which pixels are close to which pixels in order to get a meaning out of a group of pixels. For language, the numbers fed to the computer should not just be numbers: they should encode meaning. Embeddings map any text to a fixed size vector, say 50 numbers one after another, in a way that the numbers of similar words are also very similar. The embedding vector of gray and black are therefore very similar.

Historically, embeddings have been applied to words. That is, the word \emph{black} has the same embedding vector regardless of the context, where it might be used to talk about colors, the sky, or race. This creates some problems since words can have different meanings depending on the context. After BERT, the chosen method has been subword embeddings, applying the mapping to parts of the word instead of the whole. For example, the word \emph{extraterrestrial} would be first segmented into \emph{extra-terrestr-ial}, and each unit would get assigned a different embedding. There have been many methods to obtain this segmentation, the most popular one being BPE~\cite{sennrich2015neural}.

This thesis has explores he details of the BPE algorithm, coded it from scratch to deeper understand it, and replicated its results. Some improvements have been done to parts of the algorithm used in the original paper, which are presented in this thesis. A newer version of BPE, published in 2019, has also been explored, analyzed, coded and its results replicated. Some of the hyperparameters used in this method have been tweaked and its performance slightly improved. BPE-dropout~\cite{provilkov2019bpedropout} improves the performance of BPE, which has been shown in this thesis for English-German and also for other language pairs, confirming its consistency. Moreover, the most novel addition of this thesis, is applying BPE without any word boundaries.

In English and Indo-European languages, the smallest unit in language that preserves meaning is the word. Given a sentence with 5 words, it is possible to assign one meaning to each word, and obtain the meaning of the sentence by uniting the meanings of the words. But in other languages where not words, but symbols are used, the whitespace between words does not have the same defining meaning as in Indo-European languages, and perhaps it would make sense to apply embeddings to a group of whitespace-separated symbols.

As a result, this thesis extends BPE to perform also in the case where multiple-word units are allowed. The evaluation method chosen to observe the performance of BPE is not prepared to deal with multi-word units, and the thesis shows that the performance is lower in this case. However, for a specific combination of hyperparameters, the results are comparable to those of BPE using whitespace as word boundary. Finally, it is also shown that BPE-dropout outperforms BPE also in this new case of no word boundaries.
