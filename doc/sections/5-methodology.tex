% Template for a Thesis
%
% 5-methodology.tex
%
% Methodology

\chapter{Methodology}\label{ch:methodology}

This chapter explains the technical content of this thesis in broad strokes, the methodology used and the general idea of each method employed. For a more in-depth analysis and code snippets and explanations, refer to the Development chapter~\ref{ch:development}. The results, plots and graphs are displayed in the Results chapter~\ref{ch:results}. This thesis, first of all, aims to replicate the results of BPE and BPE dropout.

\section{Replication of BPE}

Using Sennrich et al.'s~\cite{sennrich2015neural} \href{https://github.com/rsennrich/subword-nmt/}{code on Github}, the first goal of the thesis is to replicate the BPE algorithm, and gauge how good the BPE units are by using an alignment algorithm and matching it against a gold standard. For that, these steps are undertaken:

\begin{enumerate}
	\item Write learn-BPE from corpus algorithm
	\item Write apply-BPE to corpus algorithm
	\item Write extract alignment script
	\item Write calculate alignment scores script
\end{enumerate}

The corpus employed in this thesis is a 10k sentence English-German corpus. As an excerpt of three sentences from the corpora:

\begin{quote}
	English\\
	21	The Committee on Transport and Tourism has adopted four amendments for the second reading .\\
	22	They will certainly enhance the feeling of the right of movement by EU citizens and they will also certainly benefit disabled drivers .\\
	23	The initial Commission proposal was adopted unamended by Parliament on first reading .\\\\
	German\\
	21	Der Transportausschuß hat für die zweite Lesung vier Änderungsanträge beschlossen .\\
	22	Sie werden bei den EU-Bürgern gewiß das Gefühl für das Recht auf Freizügigkeit stärken , und sie werden gewiß auch behinderten Fahrern Vorteile bringen .\\
	23	Der ursprüngliche Vorschlag der Kommission wurde vom Parlament in erster Lesung ohne Änderungen verabschiedet .
\end{quote}

Each step of the pipeline will be detailed as follows.

\subsection{Learn-BPE algorithm}\label{met:learnbpe}

Sennrich's repository's code has some additional parameters that were not relevant for a minimal implementation of the BPE algorithm, so the script was adapted. These are the steps for a minimal algorithm to learn-BPE units:

\begin{enumerate}
	\item Read corpus into tokens, parse index.
	\item Count pair frequencies.
	\item Start loop from 1 until desired vocabulary size. In this case, 10k merges.
	\begin{enumerate}
		\item Get most frequent pair.
		\item Append most frequent pair to vocabulary.
		\item Merge pair in corpus.
		\item Count pair frequencies in corpus.
	\end{enumerate}
	\item Write vocabulary to a file.
\end{enumerate}

This step of the pipeline only has to be done once for each corpus, afterwards the vocabulary can be used in different ways. But this minimal algorithm, since it has to count all the pairs in the whole corpus in each iteration, takes a long time. One of the improvements of this thesis is optimizing the runtime of this algorithm, which will be explained in the subsequent sections of this chapter. With the given corpus, these are the 10 most frequent merges in the English language: \_t h, \_th e, o n, r e, t i, e n, e r, i n, i s, n d. And the 10 most frequent merges in German: e n, e r, c h, i e, e i, n d, n g, \_d ie, s t, i ch. Throughout this thesis, the special symbol \_ has been used to mark the beginning of each word. Therefore, \emph{\_t} only applies to those \emph{t} characters that occur in word beginnings, not anywhere else in the word.

\subsection{Apply-BPE algorithm}\label{met:applybpe}

The learnt vocabulary can be applied to a corpus. In the case of this thesis, this corpus is the same as the one used to learn the BPE units. Different num\_merges are declared. For example, for 500 merges, only the first 500 merges of the vocabulary are taken, and there are barely any meaningful text segments. For bigger merge values, more and more subword units get merged. In this thesis, merges range from 500 up to 8000. These are the steps for applying BPE merges to a corpus:

\begin{enumerate}
	\item Load data, corpus and BPE vocabulary.
	\item Start loop for all numbers of merges: 500 merges, 2000 merges, 8000 merges.
	\begin{enumerate}
		\item Start loop from 1 until desired amount of merges: from 1 to 500 for example.
		\begin{enumerate}
			\item Merge the current most frequent pair in corpus.
		\end{enumerate}
		\item Write merged corpus to .bpe file.
	\end{enumerate}
\end{enumerate}

\clearpage
This is what the excerpts from the example corpora above look like after 100 merges, that is, after the 100 most common units in the language have been merged:

\begin{quote}
	English\\
	\_the \_Comm it t e e \_on \_T ran s p ort \_and \_T ouris m \_has \_a d o p t ed \_f our \_a m end ment s \_for \_the \_sec ond \_re a d ing \_.\\
	\_the y \_w ill \_cer t a in ly \_en h an ce \_the \_f e e ling \_of \_the \_ri g h t \_of \_m o ve ment \_b y \_E U \_citi z en s \_and \_the y \_w ill \_al s o \_cer t a in ly \_ben e f it \_d is a bled \_d ri ver s \_.\\
	\_the \_in iti al \_Commission \_pro p o s al \_w as \_a d o p t ed \_u n a m end ed \_b y \_P ar li a ment \_on \_f ir st \_re a d ing \_.\\\\
	German\\
	\_der \_T rans p or t a ussch u ß \_h at \_für \_die \_z w eite \_L es ung \_v ier \_Ä nder ung s ant r ä ge \_besch l o ss en \_.\\
	\_s ie \_wer den \_bei \_den \_E U - B ür ger n \_ge w i ß \_das \_G e f ü h l \_für \_das \_Re ch t \_auf \_F r ei z ü g i g k eit \_st ä r k en \_, \_und \_s ie \_wer den \_ge w i ß \_au ch \_beh inder ten \_F a hr er n \_V or tei l e \_b r ingen \_.\\
	\_der \_ur s p r ü ng lich e \_V or sch la g \_der \_Ko mm iss ion \_w ur de \_v o m \_P ar la ment \_in \_er ster \_L es ung \_o h n e \_Ä nder ungen \_vera b sch ie det \_.
\end{quote}

In English, \emph{\_the}, \emph{\_and}, \emph{ment} and other very common words and affix/suffixes have been merged. As for German, we can see very common words being merged such as \emph{\_die}, \emph{\_das}, \emph{\_bei} and so on. The same sentences after 1000 merges:

\begin{quote}
	English\\
	\_the \_Committee \_on \_T rans p ort \_and \_T ourism \_has \_adop ted \_four \_amendments \_for \_the \_second \_reading \_.\\
	\_they \_will \_certain ly \_en h ance \_the \_feeling \_of \_the \_right \_of \_mo vement \_by \_EU \_citizens \_and \_they \_will \_also \_certain ly \_bene f it \_dis abled \_d rivers \_.\\
	\_the \_initi al \_Commission \_proposal \_was \_adop ted \_un amended \_by \_Parliament \_on \_first \_reading \_.\\\\
	German\\
	\_der \_T ransp ort aussch uß \_hat \_für \_die \_zweite \_L esung \_v ier \_Änderungsant räge \_beschlossen \_.\\
	\_sie \_werden \_bei \_den \_EU - B ür gern \_gew iß \_das \_Ge fü hl \_für \_das \_Recht \_auf \_Freiz ü g igkeit \_st är ken \_, \_und \_sie \_werden \_gew iß \_auch \_beh inderten \_F ahrern \_Vorteile \_b ringen \_.\\
	\_der \_ur sp rüngliche \_Vorschlag \_der \_Kommission \_wurde \_vom \_Parlament \_in \_erster \_L esung \_ohne \_Änderungen \_verab schiedet \_.
\end{quote}

\clearpage
We can see bigger subwords being merged, such as \emph{reading}, \emph{first}, \emph{zweite} and so on. And the sentences after 4000 merges:

\begin{quote}
	English\\
	\_the \_Committee \_on \_Transport \_and \_Tourism \_has \_adopted \_four \_amendments \_for \_the \_second \_reading \_.\\
	\_they \_will \_certainly \_enhance \_the \_feeling \_of \_the \_right \_of \_movement \_by \_EU \_citizens \_and \_they \_will \_also \_certainly \_benefit \_dis abled \_drivers \_.\\
	\_the \_initial \_Commission \_proposal \_was \_adopted \_un amended \_by \_Parliament \_on \_first \_reading \_.\\\\
	German\\
	\_der \_T ransp ortausschuß \_hat \_für \_die \_zweite \_Lesung \_vier \_Änderungsanträge \_beschlossen \_.\\
	\_sie \_werden \_bei \_den \_EU-B ür gern \_gew iß \_das \_Ge fü hl \_für \_das \_Recht \_auf \_Freizügigkeit \_stärken \_, \_und \_sie \_werden \_gew iß \_auch \_behinderten \_Fahrern \_Vorteile \_bringen \_.\\
	\_der \_ursprüngliche \_Vorschlag \_der \_Kommission \_wurde \_vom \_Parlament \_in \_erster \_Lesung \_ohne \_Änderungen \_verabschiedet \_.
\end{quote}

Most words are merged, except \emph{\_dis abled} in English, and \emph{\_T ransp ortausschuß} in German for instance. At this point, the corpus is not composed of words anymore, but rather of subwords.

\subsection{Extract alignments}\label{subsec:extractalign}

To evaluate if the BPE units are good, bilingual corpora are aligned  and then compared against a gold standard. The motivation behind alignment and how it works can be found in~\ref{ch:translation}. The challenge here lies in the fact that if the BPEs are of good quality, the alignment algorithm will align subword items correctly, and therefore in the subword-to-word mapping, the word alignments will have a high score relative to the gold standard.

On the first step, alignment, two algorithms have been used, namely \emph{Fastalign} and \emph{Eflomal}. The software installation guides can be found in the development section~\ref{ch:development}. These algorithms take English text and German as input and create an alignment file as output. For the example above:

\begin{quote}
	English sentence: The Committee on Transport and Tourism has adopted four amendments for the second reading .\\
	German sentence: Der Transportausschuß hat für die zweite Lesung vier Änderungsanträge beschlossen .\\
	Alignment output: 0-0 1-1 2-1 3-1 4-1 5-1 6-2 7-9 8-7 9-8 10-3 11-4 12-5 13-6 14-10\\

	English sentence: The initial Commission proposal was adopted unamended by Parliament on first reading .\\
	German sentence: Der ursprüngliche Vorschlag der Kommission wurde vom Parlament in erster Lesung ohne Änderungen verabschiedet .\\
	Alignment output: 0-0 1-1 2-4 3-2 3-3 4-5 5-13 6-11 6-12 7-6 8-7 9-8 10-9 11-10 12-14
\end{quote}

\clearpage
Many words have one-to-one alignment, such as \emph{four}-\emph{vier} and \emph{adopted}-\emph{verabschiedet}. Some others have many-to-one alignments, such as \emph{Committee on Transport and Tourism}-\emph{Transportausschuß} and one-to-many alignments such as \emph{unamended}-\emph{ohne Änderungen}. In our case however, the input files are not composed by words, but rather by subwords. And the alignments are done among subwords. Using the example above but with subwords instead of words:

\begin{quote}
	English sentence: \_the \_Committee \_on \_Transport \_and \_Tourism \_has \_adopted \_four \_amendments \_for \_the \_second \_reading \_.\\
	German sentence: \_der \_T ransp ortausschuß \_hat \_für \_die \_zweite \_Lesung \_vier \_Änderungsanträge \_beschlossen \_.\\
	Alignment output: 0-0 1-1 2-1 3-1 4-1 5-1 1-2 2-2 3-2 4-2 5-2 1-3 2-3 3-3 4-3 5-3 6-4 7-11 8-9 9-10 10-5 11-6 12-7 13-8 14-12
\end{quote}

Now there are subword alignments as opposed to word alignments. For instance, since the German word \emph{Transportausschuß} is divided into three words, namely \emph{T ransp ortausschuß}, the many-to-one alignment from the previous case is now a many-to-many alignment. The number of alignment has grown from last example. Because the gold standard against which the system is being evaluated consists of word alignments, it is necessary to map subword alignments into word alignments, which is the second step in this algorithm. Using English and German corpora and the alignment file as input files, this algorithm gives a word alignment file as output.

\begin{quote}
	English sentence: \_the \_Committee \_on \_Transport \_and \_Tourism \_has \_adopted \_four \_amendments \_for \_the \_second \_reading \_.\\
	German sentence: \_der \_T ransp ortausschuß \_hat \_für \_die \_zweite \_Lesung \_vier \_Änderungsanträge \_beschlossen \_.\\
	Subword alignment: 0-0 1-1 2-1 3-1 4-1 5-1 1-2 2-2 3-2 4-2 5-2 1-3 2-3 3-3 4-3 5-3 6-4 7-11 8-9 9-10 10-5 11-6 12-7 13-8 14-12\\
	Word alignment output: 0-0 1-1 2-1 3-1 4-1 5-1 6-2 7-9 8-7 9-8 10-3 11-4 12-5 13-6 14-10
\end{quote}

\subsection{Calculate alignment scores}

In the final step, the alignment scores are computed against the gold standard. After loading the gold dataset, each alignment file is matched against this dataset, obtaining precision, recall, F1 score and AER metrics. In the case of 100 learnt symbols, there will be an associated score and so on for other numbers of learnt symbols. Additionally, the gold standard's scores are also computed as a baseline. To make it more visual, the scores are plotted and saved into a \emph{.png} image file as well as \emph{.csv} file with the exact numbers.

\clearpage
\section{Replication of BPE-dropout}\label{met:replbpedrop}

The difference between BPE-dropout and standard BPE is the fact that some merges do not take place. Based on this random factor, each time this system is carried out, new BPE merges are created. For example, if the most merge in English \emph{(\_t, h)} is not merged, the resulting file of BPE merges is vastly different than if the 10th most frequent merge does not take place. In order to obtain different instances, the dropout algorithm is run a number of times, in this case 10 times, the alignments extracted 10 times, and then the alignments aggregated. Regarding each specific step, the algorithm to create the merge list (learn\_bpe) remains unchanged, the first slight change occurs when applying the BPE algorithm to the corpus.

In the apply-BPE algorithm, a random variable is created for every merge: if it falls below a threshold, the dropout threshold, the merge in question is discarded. Apart from this, the whole algorithm is run a number of times, creating a number of BPE files, usually ten throughout this thesis. When extracting alignments, since now there are ten BPE files instead of a single one, the whole algorithm is run ten times, and alignments for all numbers of merges multiplied by all dropout repetitions are saved. At this point, there are ten alignments for each sentence. Which alignments to pick, is the next step to be resolved. Three methods are selected:

\begin{itemize}
	\item Create the union of all alignments.
	\item Create the intersection of all alignments.
	\item Create a threshold parameter, for example 0.5. If an alignment is present in 50\% of all alignments for that sentence, it is added to the aggregated file.
\end{itemize}

To illustrate this with a example. Files 1, 2 and 3 are three alignment files for a given sentence pair.

\begin{itemize}
	\item File 1: 0-0 0-1 1-1 1-2 2-3
	\item File 2: 0-0 0-1 1-2
	\item File 3: 0-0 1-1 1-3
	\item Union file: 0-0 0-1 1-1 1-2 1-3 2-3
	\item Intersection file: 0-0
	\item Aggregated file: 0-0 0-1 1-1 1-2
\end{itemize}

As it is visible in the example, the \textbf{union} case takes all alignments into account. By brute force, possibly most of the correct alignments will be present in the alignment, yielding a high recall, but the majority of the alignments in the union file will be incorrect, that is, it will have low precision. By contrast, the \textbf{intersection} case is the opposite. The file is much shorter since only the alignments present in \emph{all} files are considered, which means that these alignments will mostly be correct. But also many of the actually correct alignments will not be present, because they might have been skipped, and therefore are not present in the intersection.

The method of creating an \textbf{aggregated file} with the threshold aims to alleviate this problem by creating a sort of middle point between union and intersection. Taking a threshold value closer to 0 will mean that almost all alignments will be accepted, and therefore the score will be closer to the score of the union. In the opposite end of the spectrum, a threshold value close to 1 means that only alignments present in most alignment files will be accepted, and this resembles the intersection. Many experiments have been done with threshold values ranging from 0.3 to 0.9, and the corresponding scores can be found in the Results chapter.

\section{Improvement of learn-BPE algorithm}\label{sec:improvlearnbpe}

One of the drawbacks of the algorithm to learn-BPE units is that every time a pair of sequences is merged in the corpus, all sequence pairs and their frequencies has to be computed from scratch. This requires iterating over all characters from each sentence in the corpus, and for each iteration, a pair of sequences gets merged.

\subsection{Updating only neighboring sequences}

In order to understand the magnitude of this computation, let us explore the step of merging a pair in the corpus. We can use this small corpus as an example:

\begin{quote}
	0	A start and the end .\\
	1 	The index of a document .\\
	2 	My name is Bob .
\end{quote}

In the foremost step, all characters are separated into individual tokens, and the beginning of the work is marked:

\begin{quote}
	0	\_A \_s t a r t \_a n d \_t h e \_e n d .\\
	1 	\_T h e \_i n d e x \_o f \_a \_d o c u m e n t .\\
	2 	\_M y \_n a m e \_i s \_B o b .
\end{quote}

For the sake of the example, let us assume that ('n', 'd') is the most frequent pair of tokens. When merging them, the corpus is altered in the following way:

\begin{quote}
	0	\_A \_s t a r t \_a nd \_t h e \_e nd .\\
	1 	\_T h e \_i nd e x \_o f \_a \_d o c u m e n t .\\
	2 	\_M y \_n a m e \_i s \_B o b .
\end{quote}

In the brute force approach, each pair's frequency is computed again: the sequence pairs ('\_T', 'h'), ('h', 'e'), etc. are all revisited and their frequencies counted from scratch. But this does not have to be the case, actually the only pairs that need to be updated are the ones surrounding ('n', 'd'). When viewing this merge from a different perspective, these are the changes that occur in the pairs of tokens:

\begin{itemize}
	\item ('\_a', 'n') in sentence 0 now becomes ('\_a', 'nd')
	\item ('\_e', 'n') in sentence 0 now becomes ('\_e', 'nd')
	\item ('i', 'n') in sentence 1 now becomes ('i', 'nd')
	\item ('d', 'e') in sentence 1 now becomes ('nd', 'e')
\end{itemize}

The pair of tokens in the word 'and' which previously was ('\_a', 'n'), now becomes ('\_a', 'nd') since 'n' and 'd' have been merged. In this instance, the pair ('\_a', 'n') no longer exists, and a new pair has been created: ('\_a', 'nd'). And so on for the rest of the tokens. As a result, only the following frequency updates must be made:

\begin{itemize}
	\item Reduce frequency of ('\_a', 'n') by 1, increase frequency of ('\_a', 'nd') by 1.
	\item Reduce frequency of ('\_e', 'n') by 1, increase frequency of ('\_e', 'nd') by 1.
	\item Reduce frequency of ('i', 'n') by 1, increase frequency of ('i', 'nd') by 1.
	\item Reduce frequency of ('d', 'e') by 1, increase frequency of ('n', 'de') by 1.
\end{itemize}

All the other pairs remain unchanged. This is the major improvement of this thesis regarding the learn-BPE algorithm, \textbf{the fact that only neighboring tokens of the merged pair need to be updated}. Now, instead of updating each pair in each sentence, it is only necessary to update the merge pair's surrounding tokens. The way to do this is by locating the merged pair in the sentence, and updating the previous and next tokens.

\subsection{Saving indexes of pairs}

If a pair is very frequent, it is safe to assume that it will be present in the majority of the sentences in the corpus. In the example above, the last sentence does not contain the ('n', 'd') pair. In a bigger corpus, the more merges are done, the rarer they become. It is therefore useful to only visit those sentences where the pair is present. If the merged pair only appears in 10\% of the corpus' sentences, it is a waste of resources to visit all sentences. This can be solved by saving the index where each pair appears. This way, each pair has its frequency associated to it, as well as a list of indexes where it is present. Creating this index list can be done in the initial step of the algorithm when the corpus is read and iterated completely, each pair's frequency computed for the first time, and the indexes recorded.

This way, when accessing the most frequent pair in the corpus, we can also access to the sentences they are present in, and iterate only those.

\section{BPE without word boundaries}

Up until this point, the underlying principle of creating BPE units is that merges between words are not considered. But given the existence of languages without any clear word boundaries, this thesis also explores what would happen if merges between different words were allowed, which has not been done before in literature, to the knowledge of the author. Regarding the algorithm pipeline, all remains the same except the algorithm to create BPE merges. Now, instead of having a special token to mark the beginning of each word, the same token is used to replace whitespace. For instance, the space mode tokenization used until now and the new no-space mode tokenization:

\clearpage
\begin{itemize}
	\item Raw sentence: The cake is delicious .
	\item Sentence after space mode tokenization: \_T h e \_c a k e \_i s \_d e l i c i o u s .
	\item Sentence after no-space mode tokenization: T h e \_ c a k e \_ i s \_ d e l i c i o u s \_ .
\end{itemize}

This way, merges with whitespace are possible, as well as merges between endings of some words and beginnings of the next. After learning BPE units using this method, these are the most common merges in English: e \_, t h, s \_, t \_, n \_, d \_, th e\_, e r, i n, y \_. The most common BPE unit between words is \emph{of\_ the\_}. As for German, the most common merges are: n \_, e r, e n\_, c h, e \_, \_ d, e i, u n, t \_, er \_. And the most common BPE unit between words is \emph{, \_da}.

Since now merges between words are possible, there are many more possible merges, and the algorithm is not as fast as in the space mode. The rest of the algorithms, namely applying BPE units to a corpus, extracting alignments, and calculating scores remain the same. However, it is important to consider that when aligning units this way, a unit containing multiple words might get aligned to a unit containing also multiple words, so the mapping subword to word is slightly altered, relative to the previous case where only parts of words were aligned.
