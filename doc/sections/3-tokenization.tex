% Template for a Thesis
%
% 3-tokenization.tex
%
% Tokenization

\chapter{Tokenization}\label{sec:tokenization}

Tokenization is the first major step in language processing. Once the corpus is obtained, the vocabulary of terms needs to be determined and for that, tokens need to be defined. Why? because the meaning of the text could easily be interpreted by analyzing the words present in the text. In NLP, we deal with meanings, and it is assumed that words have a meaning. So we want to extract words. We assign a meaning to each word.

A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.

Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens.

Tokenization is the task of splitting a phrase, sentence, paragraph, or an entire text document into smaller units called tokens, which can be individual words or terms.

Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation.~\cite{Bib:stanford_token}

Here is an example of tokenization.

\begin{quote}
    Input: Cats, felines, tigers, I ate a burger today.\\
    Output: ['Cats', 'felines', 'tigers', 'I', 'ate', 'a', 'burger', 'today', '.']
\end{quote}

Similarly, tokens can be either characters or subwords. For example, let us consider “smarter”:

    Character tokens: s-m-a-r-t-e-r
    Subword tokens: smart-er

The major question in the tokenization phase is: \textbf{what are the correct tokens to use?} In the simple example above, it looks simple: delete whitespaces and throw away punctuation characters. 

But each language has its own set of tricky cases, for example in English what is the correct approach when dealing with apostrophes for possession and contractions? Words like \emph{aren't}, \emph{Sarah's}, \emph{O'Neill}. Because of this, it is imperative to know the language of the text to be tokenized. \textit{Language identification} is the task of identifying the language of the input text. From the initial k-gram algorithms used in cryptography (Konnheim, 1981), to more modern n-gram methods (Dunning, 1994) have been used

The tokens could be words, numbers or punctuation marks. In tokenization, smaller units are created by locating word boundaries.

Conceptually, splitting on white space can also split what should be regarded as a single token. This is mostly the case with names, borrowed foreign phrases, and compounds that are sometimes written as multiple words. Splitting tokens on spaces can cause bad retrieval results. This problem can be mitigated by asking the user to write such words with a hyphen between them, but this requires user training.

\subsection{Tokenization without word boundaries}

Since there are many possible segmentations of character sequences and given the complications of dealing with different languages and the intricancies of each, there is no foolproof and/or unique tokenization. Another approach has been to abandon word-based indexing, and do all indexing from just short subsequences of characters (character -grams), regardless of whether particular sequences cross word boundaries or not.

This can be specifically beneficial in some Asian languages, where an individual symbol can resemble a syllable rather than a word or letter,  most words are short (the most common length is 2 characters), and that, given the lack of standardization of word breaking in the writing system, it is not always clear where word boundaries should be placed.

Punctuations are rare in certain languages!!

Hence, at times, each character used is taken as a token in Chinese tokenization.

\section{Motivation}

For example, Transformer based models – the State of The Art (SOTA) Deep Learning architectures in NLP – process the raw text at the token level. Similarly, the most popular deep learning architectures for NLP like RNN, GRU, and LSTM also process the raw text at the token level.


\section{Tokenization algorithm types}

Tokenization can be performed on word, character, or subword level.

\subsection{Word level tokenization}

Word level tokenization was the first type that was created, and is also the most commonly used tokenization algorithm. It splits a piece of text into individual words based on a specific delimiter, usually whitespace ' ' or other punctuation signs. Depending on this delimiter, different word-level tokens are formed.

\subsubsection{Word level algorithms}

Pretrained Word Embeddings such as Word2Vec and GloVe comes under word tokenization.

split(), regex, nltk, spacy, keras, gensim

https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/

1. Penn TreeBank:

It is a rule-based tokenization method that separates out clitics ( words that normally occur only in combination with another word, for example in I’m), keeps hyphenated words together, and separates out all punctuation.

It is mainly used with treebanks http://faculty.washington.edu/fxia/lsa2011/slides/intro\_to\_treebanks.pdf released by Linguistic Data Consortium. https://www.ldc.upenn.edu/

\subsubsection{Word level drawbacks}

One of the major issues with word tokens is dealing with Out Of Vocabulary (OOV) words. OOV words refer to the new words which are encountered at testing. These new words do not exist in the vocabulary. Hence, these methods fail in handling OOV words.

But wait – don’t jump to any conclusions yet!

    A small trick can rescue word tokenizers from OOV words. The trick is to form the vocabulary with the Top K Frequent Words and replace the rare words in training data with unknown tokens (UNK). This helps the model to learn the representation of OOV words in terms of UNK tokens
    So, during test time, any word that is not present in the vocabulary will be mapped to a UNK token. This is how we can tackle the problem of OOV in word tokenizers.
    The problem with this approach is that the entire information of the word is lost as we are mapping OOV to UNK tokens. The structure of the word might be helpful in representing the word accurately. And another issue is that every OOV word gets the same representation

Another issue with word tokens is connected to the size of the vocabulary. Generally, pre-trained models are trained on a large volume of the text corpus. So, just imagine building the vocabulary with all the unique words in such a large corpus. This explodes the vocabulary!

This opens the door to Character Tokenization.
    
\subsection{Character level tokenization}

Character Tokenization splits apiece of text into a set of characters. It overcomes the drawbacks we saw above about Word Tokenization.

    Character Tokenizers handles OOV words coherently by preserving the information of the word. It breaks down the OOV word into characters and represents the word in terms of these characters
    It also limits the size of the vocabulary. Want to talk a guess on the size of the vocabulary? 26 since the vocabulary contains a unique set of characters

https://www.lighttag.io/blog/character-level-NLP/
https://github.com/fnl/segtok
https://github.com/fnl/syntok

\subsubsection{Character level algorithms}

\subsubsection{Character level drawbacks}

Character tokens solve the OOV problem but the length of the input and output sentences increases rapidly as we are representing a sentence as a sequence of characters. As a result, it becomes challenging to learn the relationship between the characters to form meaningful words.

This brings us to another tokenization known as Subword Tokenization which is in between a Word and Character tokenization.

\subsection{Subword level tokenization}

Subword Tokenization splits the piece of text into subwords (or n-gram characters). For example, words like lower can be segmented as low-er, smartest as smart-est, and so on.

Transformed based models – the SOTA in NLP – rely on Subword Tokenization algorithms for preparing vocabulary. Now, I will discuss one of the most popular Subword Tokenization algorithm known as Byte Pair Encoding (BPE).

\subsubsection{Subword level algorithms}

BPE, WordPiece, SentencePiece. since BPE is the basis of the thesis, it deserves a section in itself.

\subsubsection{Subword level drawbacks}

...

\section{BPE}

Byte Pair Encoding (BPE) is a widely used tokenization method among transformer-based models. BPE addresses the issues of Word and Character Tokenizers:

    BPE tackles OOV effectively. It segments OOV as subwords and represents the word in terms of these subwords
    The length of input and output sentences after BPE are shorter compared to character tokenization

BPE is a word segmentation algorithm that merges the most frequently occurring character or character sequences iteratively. Here is a step by step guide to learn BPE.

Steps to learn BPE

    Split the words in the corpus into characters after appending </w>
    Initialize the vocabulary with unique characters in the corpus
    Compute the frequency of a pair of characters or character sequences in corpus
    Merge the most frequent pair in corpus
    Save the best pair to the vocabulary
    Repeat steps 3 to 5 for a certain number of iterations

Example in https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/

\subsection{Applying BPE to OOV words}

(see python code bpe\_10.py)

But, how can we represent the OOV word at test time using BPE learned operations? Any ideas? Let’s answer this question now.

    At test time, the OOV word is split into sequences of characters. Then the learned operations are applied to merge the characters into larger known symbols.

    – Neural Machine Translation of Rare Words with Subword Units, 2016

Here is a step by step procedure for representing OOV words:

    Split the OOV word into characters after appending </w>
    Compute pair of character or character sequences in a word
    Select the pairs present in the learned operations
    Merge the most frequent pair
    Repeat steps 2 and 3 until merging is possible

\subsection{BPE dropout}

\section{WordPiece}

Byte Pair Encoding falter outs on rare tokens as it merges the token combination with maximum frequency.

WordPiece Tokenization is almost similar to Byte Pair Encoding. The only difference lies in the way we merge up the tokens to produce new tokens. While in BPE, we use the max frequency of the new token for merging but in WordPiece, we also take in consideration the frequency of two tokens separately used for merging apart from the frequency of the new token. This means if we have 2 token A \& B, we will be calculated:

Score(A,B) = Frequency(A,B)/Frequency(A)*Frequency(B) and max value pair will be selected

This definitely has an edge over Byte Pair Encoding.

It might be the case that Frequency(‘so’,’ on’) is very high but their separate frequencies are also high, hence if using WordPiece , we won’t be merging ‘soon’ as the overall score might go low. Again if Frequency(‘Jag’,’gery’) might be low but if their separate frequencies are also low, we will merge to form ‘jaggery’.

4. Unigram Language Model:

Let’s see the below example:

Even if we merged ‘face’+’d’ over ‘no’+’d’, it doesn’t mean we won’t encounter ‘nod’ ever. It just means that the probability of ‘nod’ is lesser but not 0. But if we use WordPiece, we might miss this.

So, what do to now?

What we need is a model that provides us with the probability of different tokens that can be generated. This simply means if we get the word ‘nod’, we shall get probabilities for ‘no+d’, ‘n+od’,’ nod’ but with different probabilities given the case these tokens (no,d, n, od, nod) are given in the initial vocabulary.

Let’s explore the Unigram Language model now

Read more in https://medium.com/data-science-in-your-pocket/tokenization-algorithms-in-natural-language-processing-nlp-1fceab8454af

\section{{SentencePiece}}

See medium link from wordpiece section

% https://www.quora.com/NLP-What-does-it-mean-Tokenizing
% https://www.quora.com/Why-is-tokenization-important-in-NLP
% https://en.wikipedia.org/wiki/Lexical\_analysis#Tokenization
