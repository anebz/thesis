% Template for a Thesis
%
% 3-tokenization.tex
%
% Tokenization

\chapter{Tokenization}\label{sec:tokenization}

\section{Introduction}

Tokenization is the first major step in language processing. Once the corpus is obtained, before starting to process the text, because we're dealing with language, we want to understand the meaning of the text. \textbf{Tokens} in the language have a semantic meaning, be it words, phrases, symbols or other elements, whereby a meaning is assigned to each token. Tokenization is the process of breaking a stream of text up into tokens.~\cite{manning2008introduction}

The way to tokenize heavily depends on the task afterwards. Some applications might require different tokenization algorithms. Nowadays, most deep learning architectures in NLP process the raw text at the token level and as a first step, create embeddings for these tokens, which will be explained in more detail in the following sections.~\ref{subsec:wordemb} In short, the type of tokenization depends on the type of embedding. There are several, explained further in the following sections, and each has its advantages and drawbacks.~\ref{subsec:toktypes}

\paragraph{What is a token?}

A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. Here is an example of tokenization.

\begin{quote}
    Input: I ate a burger today, and it was good.\\
    Output: ['I', 'ate', 'a', 'burger', 'today', 'and', 'it', 'was', 'good']
\end{quote}

In this example, the way to obtain tokens looks simple: just locate word boundaries, split by whitespace and get the symbols, and remove the punctuation marks, since punctuation marks have no definite meaning. However, this is not always the case.

\paragraph{How to deal with punctuation marks?}

Each language has its own set of tricky cases, for example in English what is the correct approach when dealing with apostrophes for possession and contractions? For example words like \emph{aren't}, \emph{Sarah's} and \emph{O'Neill}. Because of this, it is imperative to know the language of the text to be tokenized. \textit{Language identification} is the task of identifying the language of the input text. From the initial k-gram algorithms used in cryptography (Konnheim, 1981), to more modern n-gram methods (Dunning, 1994) have been used. Once the language is known, we can follow the rules for each case and deal with punctuation marks appropriately.

\paragraph{Other types of tokens}

Additionaly, tokens can be either characters or \textbf{subwords}. For example, the word \emph{smarter}:

\begin{quote}
    Sentence: the smarter computer
    Word: the, smarter, computer\\
    Word without word boundaries: the smart, er comput, er\\
    Character tokens: t-h-e, s-m-a-r-t-e-r, c-o-m-p-u-t-e-r\\
    Subword tokens: the, smart-er, comput-er
\end{quote}

The major question in the tokenization phase is: \textit{what are the correct tokens to use?}. The following section explores these 4 types of tokenization methods and delves into the algorithms and code libraries available.

\section{Tokenization algorithm types}\label{subsec:toktypes}

\subsection{Word level tokenization}

Word level tokenization was the first tokenization type used, and is also the most common. It splits a piece of text into individual words based on word bounderies, usually a specific delimiter, mostly whitespace ' ' or other punctuation signs.

Conceptually, splitting on whitespace can also split an element which should be regarded as a single token, for example New York. This is mostly the case with names, borrowed foreign phrases, and compounds that are sometimes written as multiple words. Word level tokenization without word boundaries aims to address that problem.~\ref{subsec:wordtokwowb} on page~\pageref{subsec:wordtokwowb}

\subsubsection{Word level algorithms}

The most simple way to obtain word level tokenization is by simply splitting the sentence on the desired delimeter, whitespace usually. The \pyth{sentence.split()} function in Python or a Regex command \pyth{re.findall("[\w']+", text)} achieve this in a simple way.

The \href{https://www.nltk.org/}{natural language toolkit (NLTK)} in python provides a \href{https://www.nltk.org/api/nltk.tokenize.html}{tokenize package} which includes a \emph{word\_tokenize} function, which requires the user to give in the language of the text. If none is given, English is taken as default.

\begin{python}
from nltk.tokenize import word_tokenize
sentence = u'I spent $2 yesterday'
sentence_tokenized = word_tokenize(sentence, language='English')
>>> sentence_tokenized = ['I', 'spent', '$', '2', 'yesterday']
\end{python}

Similarly, SpaCy offers a similar functionality. It is possible to load the language model for a different language and model size.

\begin{python}
import spacy
sp = spacy.load('en_core_web_sm')
sentence = u'I spent $2 yesterday'
sentence_tokenized = sp(sentence)
>>> sentence_tokenized = ['I', 'spent', '$', '2', 'yesterday']
\end{python}

Keras also offers a similar functionality:

\begin{python}
from keras.preprocessing.text import text_to_word_sequence
sentence_tokenized = text_to_word_sequence(sentence)
\end{python}

As does Gensim:

\begin{python}
from gensim.utils import tokenize
sentence_tokenized = list(tokenize(sentence))
\end{python}

\paragraph{Word embeddings}\label{subsec:wordemb}

As stated before, the goal of tokenization is to split the text into units with meaning. Typically, each token is assigned an embedding vector: Mikolov et al., 2013~\cite{mikolov2013efficient} introduced word2vec, as a way of transforming a word into a fixed-size vector representation, as shown in the picture below.

\includegraphics[width=9cm]{figures/word_emb.png}

If viewed abstractly in its N dimensions, each word's representation is close to similar words. As a simple example:

\begin{quote}
    Word: smart, embedding: [2, 3, 1, 4]\\
    Word: intelligent, embedding: [2, 3, 2, 3]\\
    Word: stupid, embedding: [-2, -4, -1, -3]
\end{quote}

The embedding numbers are just as an example, to illustrate the distances between words. For example, \emph{smart} and \emph{intelligent} have a distance of 2, since the last 2 numbers in the vector differ by one respectively. If this was plotted in a 4 dimensional space, these words would be very close together. On the other hand, \emph{stupid} is almost the opposite of \emph{smart}. The distance in this case is much higher. In the plot, these words would be very far apart.

As well as word2vec, there are other word embedding algorithms such as GloVe or fasttext.

Thus, with word embeddings, a sentence is transformed into a sequence of embedding vectors, which is very useful for NLP tasks.

\subsubsection{Word level drawbacks}

Word embeddings have some drawbacks however. In many cases, a word can have more than one meaning: \emph{well}, for example, can be used in these 2 scenarios.

\begin{quote}
    I'm doing quite well.\\
    The well was full of water.
\end{quote}

In the first case, well is an adverb while in the second it's a noun. \emph{well}'s embedding will probably be a mixture of the two, since word embeddings don't generalize to homonyms.

Another drawback is that word embeddings aren't well equipped to deal with out of vocabulary (oov) words. Word embeddings are created with a certain vocabulary size, that is, a certain number of words are known. If afterwards a new word arrives which isn't present in the vocabulary, because it's a foreign word, or a misspelled word, it will be given an unknown <UNK> embedding, that will be the same for all unknown words. Therefore all unknown words have the same embedding, that is, the NLP task will treat all these words as if they had the same meaning. The information within these words is lost due to the mapping from OOV to UNK.

Another issue with word tokens is related to the vocabulary size. Generally, pre-trained models are trained on a large volume of the text corpus. As such, if the vocabulary is built with all the unique words in such a large corpus, it creates a huge vocabulary. This opens the door to \emph{character tokenization}, since in this case the vocabulary depends on the number of characters, which is significantly lower than the number of all different words.

These problems are not to be mistaken with tokenization problems, tokenization is merely a way to an ends, they're mostly used to create embeddings. And if embeddings from word tokens are problematic, the tokenization method is changed in order to create different tokens, in order to create other types of embeddings.
    
\subsection{Character level tokenization}

In this type of tokenization, instead of splitting a text into words, the splitting is done into characters, whereby \emph{smarter} becomes \emph{s-m-a-r-t-e-r} for instance. \href{https://github.com/karpathy/char-rnn}{Karpathy, 2015} was the first to introduce a character level language model.

OOV words, misspellings or rare words, are handled better, since they're broken down into characters and these characters are usually known. In addition, the size of the vocabulary is significantly lower, namely 26 in the simple case where only the English characters are considered, though one might as well include all ASCII characters. Zhang et al., 2015~\cite{zhang2015text}, who introduced the character CNN, consider all the alphanumeric character, in addition to puctuation marks and some special symbols.

Character level models are unrestricted in their vocabulary and see the input "as-is". Since the vocabulary is much lower, the model's performance is much better than in the word tokens case. Tokenizing sequences at the character level has shown some impressive results, as stated below.

Radfor et al., 2017~\cite{radford2017learning} from OpenAI showed that character level models can capture the semantic properties of text.

Kalchbrenner et al., 2016~\cite{kalchbrenner2016neural} from Deepmind and Leet et al., 2017~\cite{lee-etal-2017-fully} both demonstrated translation at the character level. These are particularly compelling results as the task of translation captures the semantic understanding of the underlying text. Spelling mistakes, rare words and morphological aspects are very well handled with this type of tokenization.

\subsubsection{Character level algorithms}

The previous libraries explored in the case of word embeddings (native python libraries, nltk, spacy, keras) have their own version for character level tokenization.

\subsubsection{Character level drawbacks}

When tokenizing a text at the character level, the sequences are longer, which takes longer to compute since the neural net needs to have significantly more parameters to allow the model to perform the conceptual grouping internally, instead of being handed the groups upfront.

In addition, it becomes challenging to learn the relationship between the characters to form meaningful words. Besides, there is no semantic information among characters, characters are semantically void.

Sometimes the NLP task doesn't need processing at the character level, such as when doing a sequence tagging task or name entity recognition, the character level model will output characters, which requires post processing.

As an in-betweener between word and character tokenization, subword tokenization produces subword units, smaller than words but bigger than just characters.

\subsection{Subword level tokenization}

Subword tokenization is the task of splitting the text into subwords or n-gram characters. For example, words like lower can be segmented as low-er, smartest as smart-est, and so on. In the event of an OOV word such as \emph{eorner}, this tokenizer will divide it into \emph{eorn, er} and effectivey obtain some semantic information. Very common subwords such as \emph{ing}, \emph{ion}, usually with a morphological sense, are learnt through repetition. The word \emph{unfriendly} would be split into \emph{un, friend, ly}.

Nowadays, as of 2020, the most powerful deep learning architectures are based on Transformers, introduced by Vaswani et al., 2017~\cite{vaswani2017attention} and these rely on subword tokenization algorithms to prepare the vocabulary.

\subsubsection{Subword level algorithms}

Since Transformers are a relatively architecture at the time of writing, subword tokenization is an active area of research. Nowadays three algorithms stand out: byte-pair encoding (BPE), WordPiece and SentencePiece.

Since these algorithms are the basis of the thesis, it will be explained in depth in the following section.

Huggingface tokenizers https://github.com/huggingface/tokenizers

\subsubsection{Subword level drawbacks}

TODO

\subsection{Tokenization without word boundaries}\label{subsec:wordtokwowb}

Another type of tokenization, beyond word, character or subword, is tokenization without subword boundaries. The three types of tokenization explored until now cannot create units among words, that is, they consider words separatedly.

Word and character level tokenization are deterministic, meaning that for a given word, its tokenization will always be the same. But in subword tokenization, a word like \emph{imagination} might have different tokenization, for example: \emph{im, agina, tion}, \emph{imagina, tion}, etc. It depends on the corpus that the subword tokenization algorithm is trained on, but there is no foolproff and/or unique tokenization.

Besides, when dealing with languages that don't include space tokenization, such as several Asian languages, an individual symbol can resemble a syllable rather than a word or letter. Additionally, most words are short (the most common length is 2 characters), and given the lack of standardization of word breaking in the writing system or lack of punctuation in certain languages, it is not always clear where word boundaries should be placed.

An approach to handle this has been to abandon word-based indexing, and do all indexing from just short subsequences of characters (character -grams), regardless of whether particular sequences cross word boundaries or not. Hence, at times, each character used is taken as a token in Chinese tokenization.

\section{BPE}

Byte Pair Encoding (BPE) is a widely used tokenization method among Transformer-based models. BPE addresses the issues of word and character tokenizers in the following ways: regarding OOV words, BPE segments OOV as subwords and represents the word in terms of these subwords. Regarding the longer sequence length which was one of the drawbacks of character tokenization, in this case the length of input and output sentences after BPE are shorter compared to character tokenization.

BPE is a word segmentation algorithm that merges the most frequently occurring character or character sequences iteratively. These are the steps to learn BPE:

\begin{enumerate}
    \item Split the words in the corpus into characters after appending </w>, or another special symbol to show the beginning or end of the word.
    \item Initialize the vocabulary with unique characters in the corpus
    \item Compute the frequency of each pair of characters or character sequences in corpus.
    \item Merge the most frequent pair in corpus.
    \item Save the best pair to the vocabulary.
    \item Repeat steps 3 to 5 for a certain number of iterations.
\end{enumerate}

As an example, let's consider a simple corpus with a single line, and the character \emph{'\_'} to mark the beginning of each word, so as not to merge anything between different words later. The following code shows the first step of the algorithm.

\begin{python}
tokens = []
corpus = ['this is this.']
for line in corpus:
    for word in line.split():
        tokens.append('_' + ' '.join(word))
tokens = [' '.join(sent) for sent in tokens]
>>> tokens = ['_t h i s _i s _t h i s .']
\end{python}

After that, we can initialize the vocabulary, which for now consists solely of the unique characters in the corpus. The following code snippet shows the second and third step.

\begin{python}
from collections import Counter

def get_stats(tokens):
    pairs = Counter()
    for sent in tokens:
        for word in sent[1:].split( ' _'):
            symbols = ('_' + word).split()
            for j in range(len(symbols) - 1):
                pairs[symbols[j], symbols[j+1]] += 1
    return pairs

stats = get_stats(tokens)
>>> stats = Counter({('_t', 'h'): 2, ('h', 'i'): 2, ('i', 's'): 2, ('_i', 's'): 1, ('i', 's,'): 1, ('_', 'i'): 1, ('_i', 't'): 1, ('t', '?'): 1})

most_frequent = stats.most_common(1)[0][0]
>>> most_frequent = ('_t', 'h')
\end{python}

There we can see each bigram and its frequency. For example, ('\_t', 'h') occurs twice in the corpus, and it is taken as the most frequently occurring bigram.

https://github.com/anebz/thesis/commit/11c72188bea139d82202d131372e034b6a1d8052

Example in https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/

https://www.thoughtvector.io/blog/subword-tokenization/BPE.svg

Different subword tokenization strategies change this in subtle ways - BPE scores tokens simply with their count, and uses eager encoding, while character tokenization simple has a max n-gram size of 1. This strategy sounds strikingly similar to phrase extraction using collocation, which counts word n-grams, scores them, and exports a model in similar fashion, except the sequence items are words instead of characters. And it turns out, using a phrase extraction library, you can learn an effective BPE vocabulary for languages like Japanese which aren’t whitespace separated by simply changing the word-regex to single characters, helping give this connection some credibility.

A finding like this begs the question - should BPE have a more intelligent scoring phase when choosing its vocab from n-gram counts? Mutual information as a scoring function was very valuable in achieving better phrase extraction, and it seems that it would be useful here as well. And even more than that, is there an abstract tokenizer we can define that fulfills each of these strategies and more via hyperparameters like n-gram length, scoring strategy, etc? Could this abstract tokenizer have a common implementation across platforms like Tensorflow, PyTorch, the web, etc?

\paragraph{Applying BPE to OOV words}

(see python code bpe\_10.py)

But, how can we represent the OOV word at test time using BPE learned operations? Any ideas? Let’s answer this question now.

    At test time, the OOV word is split into sequences of characters. Then the learned operations are applied to merge the characters into larger known symbols.

    – Neural Machine Translation of Rare Words with Subword Units, 2016

Here is a step by step procedure for representing OOV words:

    Split the OOV word into characters after appending </w>
    Compute pair of character or character sequences in a word
    Select the pairs present in the learned operations
    Merge the most frequent pair
    Repeat steps 2 and 3 until merging is possible

\subsection{BPE dropout}

...

\section{WordPiece}

https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46
https://stackoverflow.com/a/55416944/4569908

% original tokens = ["John", "Johanson", "'s",  "house"]
% bert_tokens == ["[CLS]", "john", "johan", "##son", "'", "s", "house", "[SEP]"]

Byte Pair Encoding falter outs on rare tokens as it merges the token combination with maximum frequency.

WordPiece Tokenization is almost similar to Byte Pair Encoding. The only difference lies in the way we merge up the tokens to produce new tokens. While in BPE, we use the max frequency of the new token for merging but in WordPiece, we also take in consideration the frequency of two tokens separately used for merging apart from the frequency of the new token. This means if we have 2 token A \& B, we will be calculated:

Score(A,B) = Frequency(A,B)/Frequency(A)*Frequency(B) and max value pair will be selected

This definitely has an edge over Byte Pair Encoding.

It might be the case that Frequency(‘so’,’ on’) is very high but their separate frequencies are also high, hence if using WordPiece , we won’t be merging ‘soon’ as the overall score might go low. Again if Frequency(‘Jag’,’gery’) might be low but if their separate frequencies are also low, we will merge to form ‘jaggery’.

4. Unigram Language Model:

Let’s see the below example:

Even if we merged ‘face’+’d’ over ‘no’+’d’, it doesn’t mean we won’t encounter ‘nod’ ever. It just means that the probability of ‘nod’ is lesser but not 0. But if we use WordPiece, we might miss this.

So, what do to now?

What we need is a model that provides us with the probability of different tokens that can be generated. This simply means if we get the word ‘nod’, we shall get probabilities for ‘no+d’, ‘n+od’,’ nod’ but with different probabilities given the case these tokens (no,d, n, od, nod) are given in the initial vocabulary.

Let’s explore the Unigram Language model now

Read more in https://medium.com/data-science-in-your-pocket/tokenization-algorithms-in-natural-language-processing-nlp-1fceab8454af

\section{SentencePiece}

https://github.com/google/sentencepiece

Sentencepiece is a less common, but very beautiful strategy for subword tokenization. While conceptually similar to BPE, it strays from the eager encoding strategy to allow it to achieve higher quality tokenization and reduce error induced by location-dependence seen in BPE. The main difference is how Sentencepiece views ambiguity in character grouping, seeing it as a source of regularization for downstream models during training, as well as its use of a simple language model to evaluate the most likely character groupings instead of eagerly picking the longest recognized string like BPE does. This results in very high quality tokenization, but comes at great cost to performance, at times making it the slowest part of an NLP pipeline. While the assumption of ambiguity in tokenization seems natural, it appears the performance trade-off is not worth it, as Google itself opted not to use this strategy in their BERT language model.

https://www.thoughtvector.io/blog/subword-tokenization/Sentencepiece.svg

See medium link from wordpiece section

% https://www.quora.com/NLP-What-does-it-mean-Tokenizing
% https://www.quora.com/Why-is-tokenization-important-in-NLP
% https://en.wikipedia.org/wiki/Lexical\_analysis#Tokenization
